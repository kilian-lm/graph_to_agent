import os
import json
from google.cloud import bigquery
from google.oauth2.service_account import Credentials
from dotenv import load_dotenv
from google.api_core.exceptions import NotFound
import logging
from google.oauth2.service_account import Credentials
from google.oauth2 import service_account
from google.cloud import bigquery
import json
import datetime
import requests
import inspect
import re

import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from collections import defaultdict

from logger.CustomLogger import CustomLogger
from controllers.BigQueryHandler import BigQueryHandler
from sql_queries.adjacency_matrix_query import ADJACENCY_MATRIX_QUERY
from sql_queries.edges_query import EDGES_QUERY
from sql_queries.nodes_query import NODES_QUERY
from sql_queries.layer_find_variable import LAYER_FIND_VARIABLE

load_dotenv()

json_graph_data = """
{
  "nodes": [
    {
      "id": "07537a68-1c7e-4edb-a72f-2d82015c490f",
      "label": "Understood! As I'm an expert in the .puml syntax i will correct it"
    },
    {
      "id": "1cc45118-72ee-4efe-95d8-06e8c02fb4c0",
      "label": "The following is a .puml content generated by an agent. Please critically review it and correct any mistakes, especially ensuring it strictly adheres to .puml syntax and does not contain any elements from other diagramming languages like Mermaid"
    },
    {
      "id": "2e419e7e-a540-4c9a-af4e-5110e54fad96",
      "label": "system"
    },
    {
      "id": "757e7439-08f8-4cea-afac-c25b01167d32",
      "label": "user"
    },
    {
      "id": "c7d1c0a4-6365-44d6-be0c-bd3fc5436b85",
      "label": "user"
    },
    {
      "id": "eac6de73-9726-43b7-9441-f8e319a972e6",
      "label": "@variable_1_2"
    },
    {
      "id": "copied-1699797991293-eac6de73-9726-43b7-9441-f8e319a972e6",
      "label": "sequenceDiagramAlice->>John: Hello John, how are you?John-->>Alice: Great!Alice-)John: See you later!"
    },
    {
      "id": "copied-1699889663524-copied-1699797991293-c7d1c0a4-6365-44d6-be0c-bd3fc5436b85",
      "label": "user"
    },
    {
      "id": "copied-1699889663524-copied-1699797991293-07537a68-1c7e-4edb-a72f-2d82015c490f",
      "label": "Understood! , I'm agent-'Deductive Reasoning', solving problems like you just described. Please provide the problem-space for me to navigate it best as possible..."
    },
    {
      "id": "copied-1699889663524-copied-1699797991293-1cc45118-72ee-4efe-95d8-06e8c02fb4c0",
      "label": "You're agent-'Deductive Reasoning'. You're one agent out of 7 who try to model problem-spaces and suggest solutions based on logical reasoning, similar to the detectives of The Poisoned Chocolates Case. Your strength lies in deriving specific conclusions from general hypotheses. Utilize schemas like Modus Ponens, Modus Tollens, Hypothetical Syllogism, and Disjunctive Syllogism., in order to solve problems, you use one of the following schemas ['Modus Ponens', 'Modus Tollens', 'Hypothetical Syllogism', 'Disjunctive Syllogism']"
    },
    {
      "id": "copied-1699889663524-copied-1699797991293-2e419e7e-a540-4c9a-af4e-5110e54fad96",
      "label": "system"
    },
    {
      "id": "copied-1699889663524-copied-1699797991293-757e7439-08f8-4cea-afac-c25b01167d32",
      "label": "user"
    },
    {
      "id": "copied-1699890186553-copied-1699889663524-copied-1699797991293-eac6de73-9726-43b7-9441-f8e319a972e6",
      "label": "How would you model following problem-space?: There was a attack of the Palestinien sided group Hamas on Israel. Now Israel is bombing Gaza with heavy civiliens casualties. There is a total 'cleaning' of the Hamas in Gaza planned by Isralien-Army. There is a high danger that the whole region will fall into war.."
    },
    {
      "id": "copied-1699890186553-copied-1699797991293-07537a68-1c7e-4edb-a72f-2d82015c490f",
      "label": "Understood! I will review following .mmd"
    },
    {
      "id": "copied-1699890186553-copied-1699797991293-1cc45118-72ee-4efe-95d8-06e8c02fb4c0",
      "label": "You're an expert in mermeid .mmd and you nedd to review following .mmd"
    },
    {
      "id": "copied-1699890186553-copied-1699797991293-2e419e7e-a540-4c9a-af4e-5110e54fad96",
      "label": "system"
    },
    {
      "id": "copied-1699890186553-copied-1699797991293-757e7439-08f8-4cea-afac-c25b01167d32",
      "label": "user"
    },
    {
      "id": "copied-1699890186553-copied-1699797991293-c7d1c0a4-6365-44d6-be0c-bd3fc5436b85",
      "label": "user"
    },
    {
      "id": "234ec0c2-3d02-4ef5-9fb1-7adaeb58a1b6",
      "x": 1626.674810293892,
      "y": -632.9703327332509,
      "label": "@varibale_1_1"
    }
  ],
  "edges": [
    {
      "from": "07537a68-1c7e-4edb-a72f-2d82015c490f",
      "id": "67194bdc-f1f3-417f-9778-4d163c8b82d1",
      "to": "757e7439-08f8-4cea-afac-c25b01167d32"
    },
    {
      "from": "1cc45118-72ee-4efe-95d8-06e8c02fb4c0",
      "id": "1329a8be-e4e2-42fd-bdb6-2057f9c320d3",
      "to": "2e419e7e-a540-4c9a-af4e-5110e54fad96"
    },
    {
      "from": "2e419e7e-a540-4c9a-af4e-5110e54fad96",
      "id": "33312b2e-b683-4489-b471-e2d1ca03d21a",
      "to": "07537a68-1c7e-4edb-a72f-2d82015c490f"
    },
    {
      "from": "757e7439-08f8-4cea-afac-c25b01167d32",
      "id": "f5b47e5e-4121-44a3-8b29-97bfe2069148",
      "to": "eac6de73-9726-43b7-9441-f8e319a972e6"
    },
    {
      "from": "c7d1c0a4-6365-44d6-be0c-bd3fc5436b85",
      "id": "f4e2015e-e7f1-4e03-b3c6-ee82986533ca",
      "to": "1cc45118-72ee-4efe-95d8-06e8c02fb4c0"
    },
    {
      "from": "copied-1699889663524-copied-1699797991293-c7d1c0a4-6365-44d6-be0c-bd3fc5436b85",
      "id": "copied-1699889663524-copied-1699797991293-f4e2015e-e7f1-4e03-b3c6-ee82986533ca",
      "to": "copied-1699889663524-copied-1699797991293-1cc45118-72ee-4efe-95d8-06e8c02fb4c0"
    },
    {
      "from": "copied-1699889663524-copied-1699797991293-07537a68-1c7e-4edb-a72f-2d82015c490f",
      "id": "copied-1699889663524-copied-1699797991293-67194bdc-f1f3-417f-9778-4d163c8b82d1",
      "to": "copied-1699889663524-copied-1699797991293-757e7439-08f8-4cea-afac-c25b01167d32"
    },
    {
      "from": "copied-1699889663524-copied-1699797991293-2e419e7e-a540-4c9a-af4e-5110e54fad96",
      "id": "copied-1699889663524-copied-1699797991293-33312b2e-b683-4489-b471-e2d1ca03d21a",
      "to": "copied-1699889663524-copied-1699797991293-07537a68-1c7e-4edb-a72f-2d82015c490f"
    },
    {
      "from": "copied-1699889663524-copied-1699797991293-1cc45118-72ee-4efe-95d8-06e8c02fb4c0",
      "id": "copied-1699889663524-copied-1699797991293-1329a8be-e4e2-42fd-bdb6-2057f9c320d3",
      "to": "copied-1699889663524-copied-1699797991293-2e419e7e-a540-4c9a-af4e-5110e54fad96"
    },
    {
      "from": "copied-1699889663524-copied-1699797991293-757e7439-08f8-4cea-afac-c25b01167d32",
      "id": "copied-1699890186553-copied-1699889663524-copied-1699797991293-f5b47e5e-4121-44a3-8b29-97bfe2069148",
      "to": "copied-1699890186553-copied-1699889663524-copied-1699797991293-eac6de73-9726-43b7-9441-f8e319a972e6"
    },
    {
      "from": "copied-1699890186553-copied-1699797991293-07537a68-1c7e-4edb-a72f-2d82015c490f",
      "id": "copied-1699890186553-copied-1699797991293-67194bdc-f1f3-417f-9778-4d163c8b82d1",
      "to": "copied-1699890186553-copied-1699797991293-757e7439-08f8-4cea-afac-c25b01167d32"
    },
    {
      "from": "copied-1699890186553-copied-1699797991293-2e419e7e-a540-4c9a-af4e-5110e54fad96",
      "id": "copied-1699890186553-copied-1699797991293-33312b2e-b683-4489-b471-e2d1ca03d21a",
      "to": "copied-1699890186553-copied-1699797991293-07537a68-1c7e-4edb-a72f-2d82015c490f"
    },
    {
      "from": "copied-1699890186553-copied-1699797991293-1cc45118-72ee-4efe-95d8-06e8c02fb4c0",
      "id": "copied-1699890186553-copied-1699797991293-1329a8be-e4e2-42fd-bdb6-2057f9c320d3",
      "to": "copied-1699890186553-copied-1699797991293-2e419e7e-a540-4c9a-af4e-5110e54fad96"
    },
    {
      "from": "copied-1699890186553-copied-1699797991293-c7d1c0a4-6365-44d6-be0c-bd3fc5436b85",
      "id": "copied-1699890186553-copied-1699797991293-f4e2015e-e7f1-4e03-b3c6-ee82986533ca",
      "to": "copied-1699890186553-copied-1699797991293-1cc45118-72ee-4efe-95d8-06e8c02fb4c0"
    },
    {
      "from": "copied-1699890186553-copied-1699797991293-757e7439-08f8-4cea-afac-c25b01167d32",
      "id": "copied-1699890186553-copied-1699797991293-f5b47e5e-4121-44a3-8b29-97bfe2069148",
      "to": "copied-1699797991293-eac6de73-9726-43b7-9441-f8e319a972e6"
    },
    {
      "from": "234ec0c2-3d02-4ef5-9fb1-7adaeb58a1b6",
      "to": "copied-1699890186553-copied-1699889663524-copied-1699797991293-eac6de73-9726-43b7-9441-f8e319a972e6",
      "id": "218594c0-41f7-4e0b-bea6-de7aa2349017"
    }
  ]
}

"""

graph_data = json.loads(json_graph_data)


class MatchedGPTCallsHandler:
    def __init__(self, timestamp, matrix_dataset_id, graph_dataset_id):
        try:
            self.timestamp = timestamp
            self.log_file = f'{self.timestamp}_matrix_layer_two.log'
            self.log_dir = './temp_log'
            self.log_level = logging.DEBUG
            self.logger = CustomLogger(self.log_file, self.log_level, self.log_dir)

            self.openai_api_key = os.getenv('OPENAI_API_KEY')
            self.openai_base_url = "https://api.openai.com/v1/chat/completions"
            self.headers = {
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {self.openai_api_key}'
            }

            self.gpt_call_log = []

            self.matrix_dataset_id = matrix_dataset_id
            self.graph_dataset_id = graph_dataset_id
            self.bq_handler = BigQueryHandler(self.timestamp, self.graph_dataset_id)

            # Additional attributes specific to matched GPT calls can be initialized here
        except json.JSONDecodeError as e:
            self.logger.error(f"Failed to parse BQ_CLIENT_SECRETS environment variable: {e}")
            raise
        except Exception as e:
            self.logger.error(f"An error occurred while initializing: {e}")
            raise

    def process_matched_gpt_calls(self, matched_calls, sorted_components):
        var_responses = {}
        for suffix, nodes in sorted_components:
            for gpt_call in matched_calls:
                # Check and process each GPT call only if all @variable terms are resolved
                if self.are_all_variables_resolved(gpt_call, var_responses):
                    updated_call, response = self.process_single_gpt_call(gpt_call, var_responses)
                    var_key = f"variable_{suffix}"
                    var_responses[var_key] = response
                    self.log_gpt_call(updated_call, response, var_key)
                else:
                    self.logger.error(f"Circuit break: Unresolved variables in GPT call: {gpt_call}")
                    raise Exception("Attempted to process a GPT call with unresolved @variable placeholders")

        return var_responses

    def are_all_variables_resolved(self, gpt_call, var_responses):
        """
        Check if all @variable placeholders in the GPT call are resolved.
        """
        for message in gpt_call['messages']:
            content = message['content']
            if any(f"@variable_{suffix}" in content for suffix in var_responses):
                return False
        return True

    def process_single_gpt_call(self, gpt_call, var_responses):
        # Circuit break if the call contains unresolved variables
        if not self.are_all_variables_resolved(gpt_call, var_responses):
            self.logger.error(f"Circuit break: Attempted to process a GPT call with unresolved variables: {gpt_call}")
            raise Exception("Circuit break: Attempted to process a GPT call with unresolved @variable placeholders")

        # If all variables are resolved, proceed with processing
        updated_call = self.update_gpt_call_with_responses(gpt_call, var_responses)
        self.logger.info(f"Processing GPT call: {updated_call}")
        response = self.get_gpt_response(updated_call)
        self.logger.info(f"GPT response received: {response}")
        return updated_call, response

    def log_gpt_call(self, request, response, var_key=None):
        log_entry = {"request": request, "response": response}
        if var_key:
            log_entry["variable_key"] = var_key
        self.gpt_call_log.append(log_entry)
        self.log_info(log_entry)

    def update_gpt_call_with_responses(self, gpt_call, var_responses):
        """Update the GPT call by replacing @var terms with responses from var_responses."""
        updated_messages = []

        # Find the highest variable suffix
        highest_suffix = 0
        for var in var_responses.keys():
            parts = var.split('_')
            suffix = int(parts[-1])
            highest_suffix = max(highest_suffix, suffix)

        # Update the GPT call with responses and replace @variable_x_y with the next higher one
        for message in gpt_call['messages']:
            content = message['content']
            for var, response in var_responses.items():
                placeholder = f'@{var}'
                if placeholder in content:
                    content = content.replace(placeholder, response)
                    self.logger.debug(f"Replaced {placeholder} with {response} in GPT call.")

                    # Replace @variable_x_y-1 with the next higher one
                    parts = var.split('_')
                    prefix = '_'.join(parts[:-1])
                    suffix = int(parts[-1])
                    if suffix == highest_suffix and suffix > 1:
                        prev_var = f'{prefix}_{suffix - 1}'
                        prev_placeholder = f'@{prev_var}'
                        if prev_placeholder in content:
                            content = content.replace(prev_placeholder, response)
                            self.logger.debug(f"Replaced {prev_placeholder} with {response} in GPT call.")

            updated_messages.append({'role': message['role'], 'content': content})

        gpt_call['messages'] = updated_messages
        self.logger.debug(f"Updated GPT call: {gpt_call}")

        return gpt_call

    def get_gpt_response(self, processed_data):
        self.logger.debug(processed_data)
        response = requests.post(self.openai_base_url, headers=self.headers, json=processed_data)
        self.logger.info(response)
        if response.status_code == 200:
            self.logger.info(response.json()["choices"][0]["message"]["content"])
            return response.json()["choices"][0]["message"]["content"]
        else:
            raise Exception(f"Error in GPT request: {response.status_code}, {response.text}")
